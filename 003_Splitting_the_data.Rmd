---
title: "003 Spending your data"
author: "by MAX KUHN AND JULIA SILGE and implemeted by Craig Slinkman"
date: "10/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Sspeding the data

There are several steps to create a useful model, including parameter estimation, model selection and tuning, and performance assessment. At the start of a new project, there is usually an initial finite pool of data available for all these tasks. How should the data be applied to these steps? The idea of data spending is an important first consideration when modeling, especially as it relates to empirical validation.

When there are copious amounts of data available, a smart strategy is to allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only. There may be questions about many modeling project steps that must be answered with limited prior knowledge. For example, one possible strategy (when both data and predictors are abundant) is to spend a specific subset of data to determine which predictors are informative, before considering parameter estimation at all.  

```
As data are reused for multiple tasks, certain risks increase, such as the risks of adding bias or large effects from methodological errors.
```  
If the initial pool of data available is not huge, there will be some overlap of how and when our data is “spent” or allocated, and a solid methodology for data spending is important. This chapter demonstrates the basics of splitting our initial pool of samples for different purposes.  

## 5.1 COMMON METHODS FOR SPLITTING DATA

The primary approach for empirical model validation is to split the existing pool of data into two distinct sets. Some observations are used to develop and optimize the model. This *training set* is usually the majority of the data. These data are a sandbox for model building where different models can be fit, feature engineering strategies are investigated, and so on. We as modeling practitioners spend the vast majority of the modeling process using the training set as the substrate to develop the model.

The other portion of the observations are placed into the *test set*. This is held in reserve until one or two models are chosen as the methods that are most likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. It is critical to only look at the test set once; otherwise, it becomes part of the modeling process.

```
How should we conduct this split of the data? This depends on the context.
```  

Suppose we allocate 80% of the data to the training set and the remaining 20% for testing. The most common method is to use simple random sampling. The rsample package has tools for making data splits such as this; the function **intial_split()** was created for this purpose. It takes the data frame as an argument as well as the proportion to be placed into training. Using the previous data frame produced by the code snippet from the summary in Section 4.2:

Dr. Slinkman's remark:  We get this data by reading $ames01.RDS$ data set in in the data directory of this project.

```{r}
require( tidyverse )                    # We live in the tidyverse ...
require( lubridate )                    # For dates ...
require( tidymodels )                   # For tidymodels ...

fp <- file.path( getwd(),               # Build file path.
                 "data",
                 "ames01.RDS" )

ames <- readRDS( fp )                   # Read data.
ames                                    # Verify data read.
```  

We now have the data.

The code for splitting the data is:



```{r}
################################################################################
# Set the random number stream using `set.seed()` so that the results can be 
# reproduced later.  This is very important if we need to debug our code.
################################################################################

set.seed(123)                           # Set the random number seed.

ames_split <-                           # Split the data.
  initial_split(ames, prob = 0.80)

ames_split                              # Display split summary.
```  


The printed information denotes the amount of data in the training set ($n=2198$), the amount in the test set is ($n=732$), and the total sample size is ($n=2,198$). nn

The object ames_split is an $rsplit$ object and only contains the partitioning information; to get the resulting data sets, we apply two more functions: 

```{r}
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)  

dim( ames_train)
dim( ames_test)
```

These objects are data frames with the same columns as the original data but only the appropriate rows for each set.



