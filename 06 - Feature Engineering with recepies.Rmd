---
title: "Chapter 6 - Feature engineering with recipes"
author: "by Mel Moreno and Mathieu Basille implemented by Craig Slinkman"
date: "10/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r preload_packages_and_data}
library(tidymodels)
data(ames)
ames <- ames %>% mutate(Sale_Price = log10(Sale_Price))

set.seed(123)
ames_split <- initial_split(ames, prob = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)
```


Feature engineering encompasses activities that reformat predictor values to make them easier for a model to use effectively. This includes transformations and encoding of the data to best represent their important characteristics. Imagine that you have two predictors in a data set that can be more effectively represented in your model of interest as a ratio; creating a new predictor from the ratio of the original two is a simple example of feature engineering.

Take the location of a house in Ames as a more involved example. There are a variety of ways that this spatial information can be exposed to a model, including neighborhood (a qualitative measure), longitude/latitude, distance to the nearest school or Iowa State University, and so on. When choosing how to encode these data in modeling, we might choose an option we believe most associated with the outcome. The original format of the data (e.g., numeric like distance versus categorical like neighborhood) is also a driving factor in feature engineering choices.

There are many other examples of preprocessing to build better features for modeling:

* Correlation between predictors can be reduced via feature extraction or the removal of some predictors.

* When some predictors have missing values, they can be imputed using a sub-model.

* Models that use variance-type measures may benefit from coercing the distribution of some skewed predictors to be symmetric by estimating a transformation.  

Feature engineering and data preprocessing can also involve reformatting required by the model. Some models use geometric distance metrics and, consequently, numeric predictors should be centered and scaled so that they are all in the same units. Otherwise, the distance values would be biased by the scale of each column.

>Authors remark: Different models have different preprocessing requirements and some, such as tree-based models, require very little preprocessing at all. Appendix A contains a small table of recommended preprocessing techniques for different models.  

recommended preprocessing techniques for different models.

In this chapter, we introduce the recipes package which you can use to combine different feature engineering and preprocessing tasks into a single object and then apply these transformations to different data sets.

This chapter uses the Ames housing data and the R objects created in the book so far, as summarized in Section 5.6.  


## 6.1 A simple recipe for the Ames housing data  

In this section, we will focus on a small subset of the predictors available in the Ames housing data:

* The neighborhood (qualitative, with 29 neighborhoods in the training set)

* The general living area (continuous, named Gr_Liv_Area)

* The year built (Year_Built)

* The type of building (Bldg_Type with values OneFam (n=1,814 ), TwoFmCon (n=45), Duplex (n=76), Twnhs (n=76), and TwnhsE (n=188 ))

>My Remark: The tidyverse package dplyr has naby useful functions for counting categorial variables.  For example, we can summarize the neighbor hoods as follows: 

```{r}
ames_train %>% 
  count( Neighborhood )
```
>If we wan  to count the number of neighborhoods we can use the following
 
```{r}
count( ames_train %>% 
  distinct( Neighborhood ) )
```
 
>Note this does not agree with author's number of 29.


Suppose that an initial ordinary linear regression model were fit to these data. Recalling that, in Chapter 4, the sale prices were pre-logged, a standard call to lm() might look like:  

```
lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type)
```  
When this function is executed, the data are converted from a data frame to a numeric design matrix (also called a model matrix) and then the least squares method is used to estimate parameters. In Section 3.2 we listed the multiple purposes of the R model formula; letâ€™s focus only on the data manipulation aspects for now. What the formula above does can be decomposed into a series of steps:

1. Sale price is defined as the outcome while neighborhood, general living area, the year built, and building type variables are all defined as predictors.

2. A log transformation is applied to the general living area predictor.

3. The neighborhood and building type columns are converted from a non-numeric format to a numeric format (since least squares requires numeric predictors).

As mentioned in Chapter 3, the formula method will apply these data manipulations to any data, including new data, that are passed to the predict() function.  

A **recipe** is also an object that defines a series of steps for data processing.  

Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them; it is only a specification of what should be done. Here is a recipe equivalent to the formula above that builds on the code summary in Section 5.6:

```
library(tidymodels) # Includes the recipes package

simple_ames <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
         data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_dummy(all_nominal())
simple_ames
#> Data Recipe
#> 
#> Inputs:
#> 
#>       role #variables
#>    outcome          1
#>  predictor          4
#> 
#> Operations:
#> 
#> Log transformation on Gr_Liv_Area
#> Dummy variables from all_nominal()
```

